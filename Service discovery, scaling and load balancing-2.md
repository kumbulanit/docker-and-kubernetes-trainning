

### **Objective**
We will:
1. **Deploy an NGINX application** in Kubernetes to serve HTTP requests.
2. **Expose the service** using Kubernetes Service for internal traffic routing.
3. **Create a load generator application** to simulate traffic to the service.
4. **Set up Horizontal Pod Autoscaling (HPA)** to automatically scale the application based on load.
5. **Monitor and test how Kubernetes distributes the load** and scales the service accordingly.

---

### **Step 1: Deploy the NGINX Application**

First, we deploy a simple NGINX application that will serve HTTP requests.

#### **1.1 Create the NGINX Deployment**

Create a file named `nginx-deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3  # Start with 3 replicas
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
```

Apply the deployment:

```bash
kubectl apply -f nginx-deployment.yaml
```

This creates an NGINX deployment with 3 Pods running.

#### **1.2 Create the NGINX Service**

Now, create a Kubernetes Service to expose the NGINX application. Create a file named `nginx-service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP  # Internal cluster access
```

Apply the service:

```bash
kubectl apply -f nginx-service.yaml
```

This service will route traffic to the NGINX Pods.

---

### **Step 2: Create a Load Generator**

Next, we will use `hey`, a simple HTTP load generator, to simulate traffic to the NGINX service.

#### **2.1 Create the Load Generator Deployment**

Create a file named `load-generator.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: load-generator
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      containers:
        - name: hey
          image: rakyll/hey
          args:
            - "-z"
            - "60s"  # Run for 60 seconds
            - "-c"
            - "10"   # Send 10 concurrent requests
            - "http://nginx-service.default.svc.cluster.local"  # The service URL
```

Apply the load generator deployment:

```bash
kubectl apply -f load-generator.yaml
```

This will create a Pod that sends 10 concurrent HTTP requests to the NGINX service for 60 seconds.

#### **2.2 Monitor the Load Generator Logs**

You can view the logs of the load generator to see the results:

```bash
kubectl logs -f deployment/load-generator
```

You'll see output similar to:

```
Requests/sec: 100
Concurrency: 10
Total Requests: 6000
```

This indicates the number of requests being generated per second.

---

### **Step 3: Set Up Horizontal Pod Autoscaling (HPA)**

We can automatically scale the NGINX deployment based on CPU usage to handle the load generated by the `hey` load generator.

#### **3.1 Create the HPA Resource**

Create a file named `hpa.yaml`:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50  # Scale when CPU usage exceeds 50%
```

Apply the HPA resource:

```bash
kubectl apply -f hpa.yaml
```

This will scale the `nginx-deployment` automatically if the CPU utilization exceeds 50%, with a minimum of 3 replicas and a maximum of 10.

#### **3.2 Monitor the Scaling**

To check the HPA status, use:

```bash
kubectl get hpa
```

This will show the scaling activity and CPU usage, like:

```
NAME          REFERENCE                    TARGETS   MINPODS   MAXPODS   REPLICAS   ADJUSTMENT
nginx-hpa     Deployment/nginx-deployment   50%/60%   3         10        3          scale down
```

---

### **Step 4: Simulate Load and Observe Scaling**

Now, as the load generator runs, Kubernetes will monitor the CPU utilization and, if the threshold is crossed, will scale the Pods. You can monitor the Pods and see the scaling in action:

```bash
kubectl get pods -l app=nginx
```

If the CPU utilization is high enough, you should see the number of replicas increase from 3 to potentially 10, based on the load.

---

### **Step 5: Cleanup**

After testing, don't forget to clean up the resources:

```bash
kubectl delete -f load-generator.yaml
kubectl delete -f nginx-service.yaml
kubectl delete -f nginx-deployment.yaml
kubectl delete -f hpa.yaml
```

---

### **What We've Achieved**

- **Service Discovery**: The NGINX service is exposed internally using a Kubernetes service. The `hey` load generator dynamically discovers this service and sends HTTP requests to it.
- **Scaling**: Horizontal Pod Autoscaling (HPA) automatically scales the NGINX deployment based on CPU utilization, ensuring the application can handle increased traffic.
- **Load Balancing**: Kubernetes load balancing ensures traffic is distributed evenly across the Pods in the deployment.

### **Next Steps**

- You can further experiment with different types of metrics for autoscaling (e.g., memory usage, custom metrics).
- You can also explore adding **external traffic load generators** to simulate real-world conditions.
